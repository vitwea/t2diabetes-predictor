# T2 Diabetes Predictor: Machine Learning Pipeline

[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Status: Active Development](https://img.shields.io/badge/Status-Active%20Development-brightgreen.svg)](#)

A comprehensive machine learning pipeline for **Type 2 Diabetes prediction** using NHANES clinical data, featuring enhanced feature engineering with HDL estimation and production-ready data preparation.

## ğŸ¯ Overview

This project implements a complete ML pipeline for diabetes prediction:

```
Raw Data 
    â†“
Feature Engineering
    â†“
Engineered Data 
    â†“
Data Cleaning 
    â†“
Clean Data 
    â†“
Data Preparation 
    â†“
ML-Ready Data
    â†“
Model Training & Evaluation
```

## âœ¨ Key Features

### ğŸ§¬ Enhanced Feature Engineering
- **23 Clinical Features** including:
  - Blood Pressure indices (4): MAP, Pulse Pressure, Systolic, Diastolic
  - Insulin Resistance (2): HOMA-IR, QUICKI
  - Anthropometric (2): Waist-Height ratio, BMI-Waist ratio
  - Advanced lipids (3): TyG, TyG-Waist, Non-HDL
  - Diet composition (4): Carb%, Fat%, Protein%, ratios
  - Metabolic Syndrome Score (complete with HDL)
  - Cardiovascular stress indicator

### ğŸ”§ Robust Data Pipeline
- **Stratified train/test split** (80/20) preserving class distribution
- **SMOTE resampling** for class balance (50/50 in training)
- **StandardScaler normalization** (Î¼=0, Ïƒ=1)
- **Inf/-inf handling** with NaN conversion before imputation
- **Parquet persistence** for reproducibility
- Comprehensive **metadata tracking** (feature names, shapes, distributions)

### ğŸ“Š Production Quality
- Full logging and error handling
- Type conversion and validation
- Outlier detection (IQR method)
- Sparse column dropping (>50% NaN)
- JSON metadata export for auditability

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8+
- pip or conda

### Setup

```bash
# Clone repository
git clone https://github.com/vitwea/t2diabetes-predictor.git
cd t2diabetes-predictor

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```


## ğŸš€ Quick Start

### 1. Feature Engineering

```bash
python -m src.data.modeling.feature_engineer
```

**Output**: `nhanes_diabetes_engineered.parquet` (57,395 Ã— 46)

Creates 23 engineered features.

### 2. Data Preparation

```bash
python -m src.modeling.main
```

**Outputs**:
- `train_prepared.parquet` (79,284 Ã— 45) - Scaled & balanced
- `test_prepared.parquet` (10,876 Ã— 45) - Scaled
- `prep_metadata.json` - Feature names & metadata

### 3. Train Models

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report

# Load prepared data
train_df = pd.read_parquet("./data/final/train_prepared.parquet")
test_df = pd.read_parquet("./data/final/test_prepared.parquet")

# Prepare X, y
X_train = train_df.drop(columns=['diabetes_dx']).values
y_train = train_df['diabetes_dx'].values
X_test = test_df.drop(columns=['diabetes_dx']).values
y_test = test_df['diabetes_dx'].values

# Train
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate
y_proba = model.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_proba)
print(f"AUC: {auc:.4f}")
print(classification_report(y_test, model.predict(X_test)))
```

## ğŸ“ Project Structure

```
t2diabetes-predictor/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ modeling/
â”‚   â”‚       â”œâ”€â”€ feature_engineer_nhanes.py    
â”‚   â”‚       â””â”€â”€ data_cleaner.py                        
â”‚   â”‚
â”‚   â”œâ”€â”€ modeling/
â”‚   â”‚   â”œâ”€â”€ main.py                    # Entry point
â”‚   â”‚   â””â”€â”€ pipeline.py                # Data preparation pipeline
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py                  # Logging utility
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_eda.ipynb      
|           
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ README_DATA.md                 # Data documentation
â”‚
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ PARQUET_SETUP_GUIDE.md            # Detailed setup guide
â”œâ”€â”€ ENHANCED_vs_ORIGINAL.md           # Feature comparison
â”œâ”€â”€ HDL_ESTIMATION_GUIDE.md           # HDL methodology
â””â”€â”€ GIT_COMMITS_PLAN.md               # Git strategy
```

## ğŸ“Š Data Overview

### Input
- **Dataset**: NHANES (National Health and Nutrition Examination Survey)
- **Samples**: 57,395
- **Raw Features**: 24 (age, glucose, BP, lipids, anthropometrics, diet)

### Process
1. **Feature Engineering**: +23 clinical features
2. **Cleaning**: Remove 3-5% outliers, validate ranges
3. **Preparation**: Stratified split â†’ Imputation â†’ SMOTE â†’ Scaling

### Output (ML-Ready)
- **Training**: 79,284 Ã— 44 (scaled, balanced 50/50)
- **Testing**: 10,876 Ã— 44 (scaled, original distribution ~8.9% positive)
- **Target**: Binary (0=No Diabetes, 1=Type 2 Diabetes)

### Features (44 total)

| Category | Count | Examples |
|----------|-------|----------|
| Blood Pressure | 4 | MAP, Pulse Pressure |
| Insulin Resistance | 2 | HOMA-IR, QUICKI |
| Anthropometric | 2 | Waist-Height, BMI-Waist |
| Glucose/Lipid | 2 | Glucose-HbA1c, TG-Chol |
| Advanced Lipids | 3 | TyG, TyG-Waist, Non-HDL |
| Diet Composition | 4 | Carb%, Fat%, Protein% |
| CV Stress | 1 | Sys/Dia ratio |
| Metabolic Syndrome | 1 | MetS Score (0-5) |

## ğŸ”¬ Methodology

### Handling Class Imbalance
Original dataset: ~8.9% positive (diabetes), ~91.1% negative

**Solution**: SMOTE (Synthetic Minority Over-sampling Technique)
- Creates synthetic samples of minority class
- Result: 50/50 balanced training set
- Prevents model bias toward majority class
- Test set maintains original distribution for realistic evaluation

### Data Scaling
`StandardScaler` normalization:
- Mean: 0, Std Dev: 1
- Fit on training data
- Applied to test data (prevents leakage)
- Essential for algorithms sensitive to feature scale (LR, SVM, NN, tree-based)

## ğŸ“ˆ Expected Performance

### Baseline (Original 24 features)
- Estimated AUC: ~0.75-0.78

### With Enhanced Features (44 features + HDL estimation)
- Expected AUC: **~0.82-0.85** (+3-7% improvement)
- Better discrimination between diabetic/non-diabetic patients
- Improved feature importance distribution

## ğŸ› ï¸ Usage Examples

### Example 1: Complete Pipeline Execution

```bash
# Feature engineering
python -m src.data.modeling.feature_engineer_nhanes_enhanced

# Data preparation
python -m src.modeling.main

# Now train models with prepared data
```

### Example 2: Inspect Prepared Data

```python
import pandas as pd
import json

# Load training data
train_df = pd.read_parquet("./data/final/train_prepared.parquet")
print(train_df.shape)  # (79284, 45)
print(train_df.describe())

# Load metadata
with open("./data/final/prep_metadata.json") as f:
    metadata = json.load(f)
    print(metadata['feature_names'])
    print(metadata['class_distribution_train'])
```

### Example 3: Train Multiple Models

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
import xgboost as xgb

models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42),
}

for name, model in models.items():
    model.fit(X_train, y_train)
    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
    print(f"{name}: AUC = {auc:.4f}")
```

## ğŸ“š Documentation

- **[PARQUET_SETUP_GUIDE.md](PARQUET_SETUP_GUIDE.md)** - Complete setup and usage guide
- **[ENHANCED_vs_ORIGINAL.md](ENHANCED_vs_ORIGINAL.md)** - Detailed feature comparison
- **[GIT_COMMITS_PLAN.md](GIT_COMMITS_PLAN.md)** - Git workflow and commits

## ğŸ› Troubleshooting

### Issue: "Input X contains infinity or a value too large"
**Solution**: Update `src/modeling/pipeline.py` to handle inf/-inf:
```python
X_train = X_train.replace([np.inf, -np.inf], np.nan)
X_test = X_test.replace([np.inf, -np.inf], np.nan)
```

### Issue: "File not found" for parquet files
**Solution**: Ensure paths in `src/modeling/main.py` match your directory structure:
```python
data_path="./data/final/nhanes_diabetes_engineered.parquet"
```

### Issue: SMOTE taking too long
**Normal behavior** for 54K+ samples. Expected: 5-10 seconds. Reduce data or `k_neighbors=3` for speed.

## ğŸ“Š Performance Metrics

The pipeline tracks:
- **Shape transformations** at each stage
- **Class distribution** (before/after SMOTE)
- **Missing value statistics**
- **Scaling parameters** (mean, std for each feature)
- **Execution time** for each phase

All saved in `prep_metadata.json` for reproducibility.

## ğŸ”„ Workflow

```
Raw NHANES Data
    â†“
[Feature Engineering] - Creates 23 clinical features
    â†“
Engineered Data (46 features)
    â†“
[Data Cleaning] - Remove outliers, validate
    â†“
Clean Data (46 features, -2% rows)
    â†“
[Data Preparation] - Split, impute, SMOTE, scale
    â†“
ML-Ready Data
    â”œâ”€â”€ X_train (79,284 Ã— 44) scaled
    â”œâ”€â”€ X_test (10,876 Ã— 44) scaled
    â”œâ”€â”€ y_train (balanced)
    â””â”€â”€ y_test (original dist.)
    â†“
[Model Training]
    â”œâ”€â”€ RandomForest
    â”œâ”€â”€ XGBoost
    â”œâ”€â”€ LogisticRegression
    â””â”€â”€ ...
    â†“
[Evaluation]
    â”œâ”€â”€ ROC-AUC
    â”œâ”€â”€ Classification Report
    â”œâ”€â”€ Feature Importance
    â””â”€â”€ Cross-validation
```

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'feat: add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

MIT License

Copyright (c) 2026 Pablo MonclÃºs

## ğŸ™ Acknowledgments

- **NHANES Dataset**: CDC/NCHS (https://www.cdc.gov/nchs/nhanes/)
- **Feature Engineering**: Clinical guidelines and epidemiological research
- **SMOTE**: Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique" (2002)
- **Friedewald Formula**: Friedewald et al., "Estimation of the Concentration of Low-Density Lipoprotein Cholesterol"

## ğŸ“ Contact & Support

For questions, issues, or suggestions:
- Open an [Issue](https://github.com/vitwea/t2diabetes-predictor/issues)
- Start a [Discussion](https://github.com/vitwea/t2diabetes-predictor/discussions)

## ğŸ¯ Roadmap

- [ ] Add cross-validation framework
- [ ] Implement hyperparameter tuning (Optuna/GridSearch)
- [ ] Add SHAP explainability
- [ ] Deploy as API (FastAPI)
- [ ] Add interpretability plots (feature importance, SHAP)
- [ ] Create interactive dashboard (Streamlit)
- [ ] Add model persistence (pickle/joblib)

## ğŸ“Š Dataset Citation

```bibtex
@misc{CDC2020,
  title={National Health and Nutrition Examination Survey (NHANES)},
  author={CDC/NCHS},
  year={2020},
  url={https://www.cdc.gov/nchs/nhanes/}
}
```

---

**Last Updated**: January 20, 2026  
**Status**: âœ… Active Development  
**Python**: 3.8+  
**Scikit-learn**: 1.0+
